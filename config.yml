# Hyperparameters
n_layers: 16
d_model: 1024
n_heads: 16
context_length: 256 # (T) # maximum context length for predictions.
batch_size_gpu: 36  # (B) # total number of batches loaded by each GPU
learning_rate: 0.0003
acc_batch_size: 525000
clip: 1.0 # gradient clipping
x0: 1125000000  # num_tokens marking end of Linear Warmup 
x1: 30000000000 # num_tokens marking end of Cosine Annealing
dropout: 0.0
tokenizer: 'gpt2'
eval_iter: 10
